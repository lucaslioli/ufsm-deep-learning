{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qctyuoZfjMYl"
   },
   "source": [
    "## Parte 2\n",
    "\n",
    "Neste exercício você irá preparar uma base de dados de imagens de expressões faciais e treinar uma regressão logística nesses dados. Para isso usaremos uma base de dados de um desafio [publicado no site Kaggle em 2013](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge), e descrito [neste artigo do arXiv.org](http://arxiv.org/abs/1307.0414). Esta base consiste em um arquivo [em formato CSV](https://pt.wikipedia.org/wiki/Comma-separated_values), onde constam milhares de imagens monocromáticas de 48x48 pixels de rostos de diferentes pessoas. Para cada imagem é dado um número inteiro de 0 a 6 que representa uma expressão seguindo esta legenda: 0=Raiva, 1=Nojo, 2=Medo, 3=Felicidade, 4=Tristeza, 5=Surpresa, 6=Neutro.\n",
    "\n",
    "Nesta base de dados há exatamente 28709 imagens destinadas para treinamento, 3589 destinadas para validação e 3589 para teste (respectivamente marcadas como 'Training', 'PublicTest' e 'PrivateTest').\n",
    "\n",
    "A regressão logística deverá ser implementada utilizando a biblioteca TensorFlow, seguindo os mesmos moldes do exercício \"Assignment 1: notMNIST\" do [curso de Deep Learning da Udacity](https://classroom.udacity.com/courses/ud730).\n",
    "\n",
    "Critérios de avaliação:\n",
    "- 1) Abrir o arquivo CSV fornecido e acessar os dados a partir do código em Python (0,5 ponto)\n",
    "- 2) Separar adequadamente dados de treinamento, validação e teste (0,5 ponto)\n",
    "- 3) Criar arrays do NumPy com as devidas dimensões e tipagem, e preenchê-las com os dados lidos do arquivo (1,0 ponto)\n",
    "- 4) Armazenar as no formato one-hot e as normalizar as imagens (1,0 ponto)\n",
    "- 5) Criar um arquivo binário do Pickle com todos os dados já formatados em arrays do NumPy (1,0 ponto)\n",
    "- 6) Ler o arquivo binário do Pickle, restaurando todos os dados já formatados em arrays do NumPy (1,0 ponto)\n",
    "- 7) Mostrar 10 exemplos aleatórios de imagens, imprimindo suas classes correspondentes (1,0 ponto)\n",
    "- 8) Implementar o Grafo da Regressão Linear no TensorFlow com as constantes, os placeholders e as variáveis, realizando as dimensões e operações corretas, incluindo erro apropriado para *one-hot encoding* (1,0 ponto)\n",
    "- 9) Implementar a Sessão no TensorFlow, fazendo o treinamento por lotes, e mostrando o erro de treinamento diminuindo (1,0 ponto)\n",
    "- 10) Mostrar o erro nos dados de validação a cada época do treinamento (1,0 ponto)\n",
    "- 11) Mostrar o erro nos dados de teste ao final de todo treinamento (1,0 ponto)\n",
    "- **Ponto Extra:** Implementar e treinar uma rede neural do tipo perceptron com duas camadas ou mais (1,0 ponto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QlngejxvjMYC"
   },
   "outputs": [],
   "source": [
    "# CONSIDERANDO A EXECUÇÃO SEPARADAMENTE DA PARTE 1\n",
    "# Importa as bibliotecas que serão utilizadas \n",
    "from __future__ import print_function\n",
    "import csv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from six.moves import range\n",
    "# Configura precisão para impressão de arrays do NumPy\n",
    "np.set_printoptions(formatter={'float':lambda x: '%+01.2f ' % x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QsIhjEbzjMYG"
   },
   "outputs": [],
   "source": [
    "# Define dicionário para as etiquetas que\n",
    "# representam cada classe\n",
    "image_size = 48\n",
    "num_labels = 7\n",
    "label_dict = {0:'Raiva', 1:'Nojo', 2:'Medo', 3:'Felicidade', 4:'Tristeza', 5:'Surpresa', 6:'Neutro' }\n",
    "\n",
    "use_dict = {1:'Training' , 2:'PublicTest' , 3:'PrivateTest'}\n",
    "\n",
    "# Define função auxiliar para retornar o nome\n",
    "# da classe correspondente para cada índice\n",
    "\n",
    "def get_label(label_array):\n",
    "    return label_dict[np.argmax(label_array)]\n",
    "\n",
    "def get_use(use_array):\n",
    "    return use_dict[use_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JSCLr_o4jMYJ"
   },
   "outputs": [],
   "source": [
    "# Define função auxiliar para leitura dos\n",
    "# dados a partir de arquivos no formato CSV\n",
    "\n",
    "def readfile(filename, total, dados):\n",
    "    with open(filename,'r') as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        i = 0\n",
    "        \n",
    "        num1 = 0 # Treino\n",
    "        num2 = 0 # Validação\n",
    "        num3 = 0 # Teste\n",
    "        \n",
    "        for row in rows:\n",
    "            if i > 0:\n",
    "                image = np.zeros((total,48*48),dtype=np.float32)\n",
    "                \n",
    "                label = np.zeros((total,7),dtype=np.float32)\n",
    "                label = (np.arange(0,7) == np.array(row[0]).astype(np.float32)) # one-hot\n",
    "                \n",
    "                data = np.array(row[1].split(' '))\n",
    "                data = data.astype(np.float32)\n",
    "                \n",
    "                image = (data / 255.0) - 0.5\n",
    "                \n",
    "                if row[2] == get_use(1):\n",
    "                    dados.train_labels[num1,:] = (label).astype(np.float32)\n",
    "                    dados.train_images[num1,:] = (image)\n",
    "                    num1 = num1+1\n",
    "                \n",
    "                elif row[2] == get_use(2):\n",
    "                    dados.valid_labels[num2,:] = (label).astype(np.float32)\n",
    "                    dados.valid_images[num2,:] = (image)\n",
    "                    num2 = num2+1\n",
    "                \n",
    "                else:\n",
    "                    dados.test_labels[num3,:] = (label)\n",
    "                    dados.test_images[num3,:] = (image)\n",
    "                    num3 = num3+1\n",
    "            \n",
    "            i = i + 1\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dados:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total = (28709+3589+3589)\n",
    "        self.train_images = np.zeros((28709,image_size*image_size),dtype=np.float32)\n",
    "        self.train_labels = np.zeros((28709,num_labels),dtype=np.float32)\n",
    "\n",
    "        self.test_images = np.zeros((3589,image_size*image_size),dtype=np.float32)\n",
    "        self.test_labels = np.zeros((3589,num_labels),dtype=np.float32)\n",
    "\n",
    "        self.valid_images = np.zeros((3589,image_size*image_size),dtype=np.float32)\n",
    "        self.valid_labels = np.zeros((3589,num_labels),dtype=np.float32)\n",
    "    \n",
    "    def ReadFil(self):\n",
    "        #preenche os arrays\n",
    "        readfile('fer2013.csv',self.total,self)\n",
    "        \n",
    "    def getTrain(self):\n",
    "        return self.train_images,self.train_labels\n",
    "    \n",
    "    def getTest(self):\n",
    "        return self.test_images,self.test_labels\n",
    "    \n",
    "    def getValid(self):\n",
    "        return self.valid_images,self.valid_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = '.' \n",
    "file_name = 'DadosQuestao2.pickle'\n",
    "\n",
    "def SavePickleDados(dados):\n",
    "    pickle_file = os.path.join(data_root, file_name)\n",
    "\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        save = { 'Dados': dados }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "def LoadPickleDados():\n",
    "    pik = pickle.load(open(file_name, 'rb'))\n",
    "    return pik['Dados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega dados \n",
    "bd = Dados()\n",
    "bd.ReadFil()\n",
    "SavePickleDados(bd)\n",
    "dados = LoadPickleDados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = dados.getTest()\n",
    "train_images, train_labels = dados.getTrain()\n",
    "valid_images, valid_labels = dados.getValid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0c4xc670jMYR",
    "outputId": "3ba71c1f-9da2-4b4e-9157-e89ea7eaa745"
   },
   "outputs": [],
   "source": [
    "# Mostra 10 exemplos\n",
    "for j in range(10):\n",
    "    i = np.random.randint(0,test_images.shape[0])\n",
    "    image = test_images[i,:].reshape((image_size,image_size))\n",
    "    print(i)\n",
    "    print(get_label(test_labels[i,:]))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_images)\n",
    "    tf_test_dataset = tf.constant(test_images)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # Regressão Lin.\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # eta/alfa/taxa_correcao\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Porcentagem de acerto\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (test_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_images[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "dpee1085_avaliacao1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
