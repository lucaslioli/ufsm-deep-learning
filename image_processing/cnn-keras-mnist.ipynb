{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Indica que será utilizada a ordem das dimensões do Theano\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Indica que os gráficos serão mostrados no jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Separa os dados e rótulos de treinamento e validação\n",
    "    (data_train, label_train), (data_test, label_test) = mnist.load_data()\n",
    "\n",
    "    # Realiza o reshape para ficar com 60k imagens, com canal 1 para escala de cinza, com tamanho 28x28\n",
    "    data_train = data_train.reshape(data_train.shape[0], 1, 28, 28).astype('float32')\n",
    "    print(\"Shape dos dados de treinamento: \", data_train.shape)\n",
    "\n",
    "    # Realiza o reshape também para os dados de validação\n",
    "    data_test = data_test.reshape(data_test.shape[0], 1, 28, 28).astype('float32')\n",
    "    print(\"Shape dos dados de validação: \", data_test.shape)\n",
    "\n",
    "    # Faz com que os dados fiquem com valores entre 0 e 1 ao invés de 0 e 255\n",
    "    data_train = data_train / 255\n",
    "    data_test = data_test / 255\n",
    "\n",
    "    # Adiciona quantidade total de categorias na posição [1] dos vetores de labels\n",
    "    label_train = np_utils.to_categorical(label_train)\n",
    "    print(\"Shape dos rótulos de treinamento: \", label_train.shape)\n",
    "    label_test = np_utils.to_categorical(label_test)\n",
    "    print(\"Shape dos rótulos de valiação: \", label_train.shape)\n",
    "\n",
    "    return data_train, label_train, data_test, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dos dados de treinamento:  (60000, 1, 28, 28)\n",
      "Shape dos dados de validação:  (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Carrega todos os dados dentro das variáveis\n",
    "data_train, label_train, data_test, label_test = load_data()\n",
    "\n",
    "# Número de classes que serão categorizadas\n",
    "num_classes = label_test.shape[1]\n",
    "\n",
    "# O model será exportado para este arquivo\n",
    "filename='mnistneuralnet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    # Instancia stack de camadas\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adiciona camada convolucional, com Kernel = 5x5\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    \n",
    "    # Adiciona camada de pooling para redução de dimensionalidade (subamostragem)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Outra camada convolucional, com Kernel = 3x3\n",
    "    model.add(Conv2D(15, (3, 3), input_shape=(1, 28, 28), activation='relu'))\n",
    "\n",
    "    # Outra camada de pooling para redução de dimensionalidade (subamostragem)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Realiza regularização para desligar alguns neurônios de aleatóriamente com probabilidade de 20%\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Converte matriz para vetor de tamanho único\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Adiciona camadas densamente conectadas com 128, 64 e 32 neurônios\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    # Última camada densa que irá classificar o mesmo número de neurôios que o número de clases\n",
    "    model.add(Dense(num_classes, activation='softmax', name='predict'))\n",
    "\n",
    "    # Compila a arquitetura para que o Keras possa formar a rede\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 24, 24)        780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 10, 10)        4065      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               48128     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "predict (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 63,639\n",
      "Trainable params: 63,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cria o modelo\n",
    "model = model()\n",
    "# Mostra um resumo da arquitetura da rede\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verifica se já existe um modelo treinado e exportado para um arquivo .h5.\n",
    "if not os.path.exists('./{}'.format(filename) ):\n",
    "    # Um novo modelo será treinado, através do método fit, caso este arquivo não exista.\n",
    "    model.fit(data_train, label_train, validation_data=(data_test, label_test), epochs=10, batch_size=200)\n",
    "    model.save_weights(filename)\n",
    "else:\n",
    "    # carrega um modelo previamente treinado\n",
    "    model.load_weights('./{}'.format(filename) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 99.09%\n"
     ]
    }
   ],
   "source": [
    "# Avalia o modelo com base nos dados de validaçãp\n",
    "scores = model.evaluate(data_test, label_test, verbose=0)\n",
    "# Exibe a acuracia do modelo criado\n",
    "print(\"\\nacc: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f576ab445c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5768e728d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lê a imagem extra para teste utilizando o OpenCV\n",
    "img_pred = cv2.imread(\"sp_noise.jpg\", 0)\n",
    "\n",
    "# Aplica threshold utilizando uma média dos picos do histograma (Otsu binarization)\n",
    "_, thresh = cv2.threshold(img_pred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Kernel de 3x3 (as imagens são geralmente de baixa resolução)\n",
    "kernel = np.ones((3,3), np.uint8)\n",
    "\n",
    "# Aplica opening na imagem para remoção de ruídos no background\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# Aplica closing na imagem para remoção de ruídos no foreground\n",
    "closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Aplica filtro utilizando kernel que valoriza o valor do ponto central, aumentando a nitidez\n",
    "kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "img_pred = cv2.filter2D(closing, -1, kernel)\n",
    "\n",
    "# Exibe a imagem\n",
    "plt.imshow(img_pred, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Verifica se a imagem está nas dimensões corretas (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    # Se não estiver, redimensiona a imagem\n",
    "    img2 = cv2.resize(img_pred, (28, 28))\n",
    "    img_pred = img2.reshape(28, 28, -1)\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28, 28, -1)\n",
    "    \n",
    "img_pred = img_pred.reshape(1, 1, 28, 28).astype('float32')\n",
    "\n",
    "img_pred = img_pred/255.0\n",
    "\n",
    "print(img_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  com confiança de  95.83%\n"
     ]
    }
   ],
   "source": [
    "# Realiza classificação a partir da imagem extra\n",
    "pred = model.predict_classes(img_pred)\n",
    "\n",
    "# Calcula a probablidade da classificação estar correta\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "\n",
    "# Imprime a probabilidade\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \" com confiança de \", pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
